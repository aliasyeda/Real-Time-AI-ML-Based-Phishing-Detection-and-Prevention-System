{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19192042-1f26-41db-bd06-e635e8e77c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (1.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (2.32.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (4.13.4)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from requests) (2025.6.15)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from beautifulsoup4) (4.14.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn requests beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db5be2c-a9f7-47a4-92a4-6fd0d5ba2485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\syeds\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9705e8e-735f-4842-a1d6-f5422d82cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b31c0915-f139-4690-b11d-e2b694c5f49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email Dataset shape: (82486, 2)\n",
      "                                       text_combined  label\n",
      "0  hpl nom may 25 2001 see attached file hplno 52...      0\n",
      "1  nom actual vols 24 th forwarded sabrae zajac h...      0\n",
      "2  enron actuals march 30 april 1 201 estimated a...      0\n",
      "3  hpl nom may 30 2001 see attached file hplno 53...      0\n",
      "4  hpl nom june 1 2001 see attached file hplno 60...      0\n",
      "URL Dataset shape: (54807, 2)\n",
      "                                                 url      Type\n",
      "0  https://docs.google.com/presentation/d/e/2PACX...  Phishing\n",
      "1    https://btttelecommunniccatiion.weeblysite.com/  Phishing\n",
      "2                        https://kq0hgp.webwave.dev/  Phishing\n",
      "3  https://brittishtele1bt-69836.getresponsesite....  Phishing\n",
      "4         https://bt-internet-105056.weeblysite.com/  Phishing\n"
     ]
    }
   ],
   "source": [
    "# Load phishing email dataset\n",
    "emails = pd.read_csv(\"phishing email.csv\")  # substitute actual filename\n",
    "print(\"Email Dataset shape:\", emails.shape)\n",
    "print(emails.head())\n",
    "\n",
    "# Standardize column names â†’ assume `EmailText` and `Label`\n",
    "emails.rename(columns={\"email_text\": \"text\", \"label\": \"label\"}, inplace=True)\n",
    "\n",
    "# Load phishing URL dataset\n",
    "urls = pd.read_csv(\"phishing URLs.csv\")  # substitute actual filename\n",
    "print(\"URL Dataset shape:\", urls.shape)\n",
    "print(urls.head())\n",
    "\n",
    "# Standardize column names â†’ assume `url` and `Label`\n",
    "urls.rename(columns={\"url\": \"text\", \"label\": \"label\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40ef796b-f7c9-48c7-a880-7c036c9be4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emails dataset columns: ['text_combined', 'label', 'source']\n",
      "URLs dataset columns: ['text', 'Type', 'source']\n"
     ]
    }
   ],
   "source": [
    "print(\"Emails dataset columns:\", emails.columns.tolist())\n",
    "print(\"URLs dataset columns:\", urls.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a909a19c-3933-4ffa-bfa5-3a7f73904915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (137293, 3)\n",
      "                                                text label source\n",
      "0  hpl nom may 25 2001 see attached file hplno 52...     0  email\n",
      "1  nom actual vols 24 th forwarded sabrae zajac h...     0  email\n",
      "2  enron actuals march 30 april 1 201 estimated a...     0  email\n",
      "3  hpl nom may 30 2001 see attached file hplno 53...     0  email\n",
      "4  hpl nom june 1 2001 see attached file hplno 60...     0  email\n",
      "label\n",
      "Phishing    54807\n",
      "1           42891\n",
      "0           39595\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# For emails dataset\n",
    "emails.rename(columns={\"text_combined\": \"text\"}, inplace=True)\n",
    "\n",
    "# For URLs dataset\n",
    "urls.rename(columns={\"Type\": \"label\"}, inplace=True)\n",
    "\n",
    "# Now combine\n",
    "df = pd.concat(\n",
    "    [emails[[\"text\",\"label\",\"source\"]],\n",
    "     urls[[\"text\",\"label\",\"source\"]]],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(\"Combined dataset shape:\", df.shape)\n",
    "print(df.head())\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "329aee63-3cf4-46ab-b758-17ebc182589f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    97698\n",
      "0    39595\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syeds\\AppData\\Local\\Temp\\ipykernel_21860\\1360821657.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label'] = df['label'].replace({\n"
     ]
    }
   ],
   "source": [
    "# Clean label values into consistent numeric form\n",
    "df['label'] = df['label'].replace({\n",
    "    \"Phishing\": 1, \n",
    "    \"phishing\": 1, \n",
    "    \"malicious\": 1,\n",
    "    \"legitimate\": 0,\n",
    "    \"benign\": 0,\n",
    "    \"good\": 0\n",
    "})\n",
    "\n",
    "# Make sure labels are integers\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c26bc1d3-1190-4ff6-bcc8-3369fb9c9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"text\"]    # all email/URL text\n",
    "y = df[\"label\"]   # phishing = 1, safe = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5f5b359-8925-4fa7-a500-666d8c3a4d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 109834\n",
      "Testing samples: 27459\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Testing samples:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce21b836-b76a-4faf-8863-acafe8bab796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tfidf shape: (109834, 10000)\n",
      "X_test_tfidf shape: (27459, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,    # top 10k words/bigrams\n",
    "    ngram_range=(1,2),     # use unigrams + bigrams\n",
    "    stop_words=\"english\"   # remove common filler words\n",
    ")\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"X_train_tfidf shape:\", X_train_tfidf.shape)\n",
    "print(\"X_test_tfidf shape:\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6ed81ca-84f3-4cd4-ba78-b999ef43e593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Accuracy: 0.9903492479697003\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      7919\n",
      "           1       0.99      0.99      0.99     19540\n",
      "\n",
      "    accuracy                           0.99     27459\n",
      "   macro avg       0.99      0.99      0.99     27459\n",
      "weighted avg       0.99      0.99      0.99     27459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train Logistic Regression\n",
    "model = LogisticRegression(max_iter=2000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "print(\"âœ… Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b8f11ac-cba4-4310-9025-2fdae565dac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ Model and vectorizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save trained model\n",
    "with open(\"phishing_combined_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Save trained TF-IDF vectorizer\n",
    "with open(\"combined_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "print(\"ðŸŽ‰ Model and vectorizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03c4b282-9632-4caa-b937-b236e9769360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations! You have won $1000. Click here to claim! â†’ ðŸš¨ Phishing\n",
      "http://secure-login.paypai.com â†’ ðŸš¨ Phishing\n",
      "Your Amazon order is shipped successfully â†’ ðŸš¨ Phishing\n",
      "https://www.microsoft.com â†’ ðŸš¨ Phishing\n"
     ]
    }
   ],
   "source": [
    "# Load back the model\n",
    "model = pickle.load(open(\"phishing_combined_model.pkl\", \"rb\"))\n",
    "vectorizer = pickle.load(open(\"combined_vectorizer.pkl\", \"rb\"))\n",
    "\n",
    "# Sanity check\n",
    "test_cases = [\n",
    "    \"Congratulations! You have won $1000. Click here to claim!\",  # phishing email\n",
    "    \"http://secure-login.paypai.com\",                             # phishing URL\n",
    "    \"Your Amazon order is shipped successfully\",                  # safe email\n",
    "    \"https://www.microsoft.com\"                                   # safe URL\n",
    "]\n",
    "\n",
    "for case in test_cases:\n",
    "    features = vectorizer.transform([case])\n",
    "    prediction = model.predict(features)[0]\n",
    "    print(f\"{case} â†’ {'ðŸš¨ Phishing' if prediction==1 else 'âœ… Safe'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff5bb859-d795-4137-a72c-897bf40297a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emails dataset columns: ['text_combined', 'label']\n",
      "URLs dataset columns: ['url', 'Type']\n",
      "âœ… Combined dataset ready: (137293, 3)\n",
      "label\n",
      "1    97698\n",
      "0    39595\n",
      "Name: count, dtype: int64\n",
      "                                                text  label source\n",
      "0  hpl nom may 25 2001 see attached file hplno 52...      0  email\n",
      "1  nom actual vols 24 th forwarded sabrae zajac h...      0  email\n",
      "2  enron actuals march 30 april 1 201 estimated a...      0  email\n",
      "3  hpl nom may 30 2001 see attached file hplno 53...      0  email\n",
      "4  hpl nom june 1 2001 see attached file hplno 60...      0  email\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syeds\\AppData\\Local\\Temp\\ipykernel_2012\\792455714.py:59: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label'] = df['label'].replace({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ Model saved as phishing_combined_model.pkl\n",
      "ðŸŽ‰ Vocabulary saved as vectorizer_vocab.json (lightweight)\n"
     ]
    }
   ],
   "source": [
    "# Extra Step: Save Lightweight Vectorizer Vocabulary for Deployment\n",
    "#\n",
    "# NOTE:\n",
    "# - The original 'combined_vectorizer.pkl' file was very large (>100MB),\n",
    "#   which cannot be uploaded easily to GitHub / Streamlit Cloud.\n",
    "# - To fix this, we export only the vocabulary of the TF-IDF vectorizer \n",
    "#   as a JSON file. This drastically reduces file size (<10MB) and \n",
    "#   keeps deployment fast.\n",
    "# - In 'app.py', we rebuild the TfidfVectorizer using this vocabulary.\n",
    "#\n",
    "# Output of this cell:\n",
    "# 1. phishing_combined_model.pkl -> Trained Logistic Regression Model\n",
    "# 2. vectorizer_vocab.json       -> TF-IDF Vocabulary (lightweight) \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle, json\n",
    "\n",
    "#  Load datasets\n",
    "emails = pd.read_csv(\"phishing email.csv\")   # <-- change filename\n",
    "urls   = pd.read_csv(\"phishing URLs.csv\")    # <-- change filename\n",
    "\n",
    "#  Print original columns\n",
    "print(\"Emails dataset columns:\", emails.columns.tolist())\n",
    "print(\"URLs dataset columns:\", urls.columns.tolist())\n",
    "\n",
    "#  Rename columns to standard names\n",
    "# For EMAILS dataset (usually 'text_combined' for email text, 'label' for class)\n",
    "if \"text_combined\" in emails.columns:\n",
    "    emails.rename(columns={\"text_combined\": \"text\"}, inplace=True)\n",
    "elif \"email_text\" in emails.columns:\n",
    "    emails.rename(columns={\"email_text\": \"text\"}, inplace=True)\n",
    "\n",
    "if \"Label\" in emails.columns:\n",
    "    emails.rename(columns={\"Label\": \"label\"}, inplace=True)\n",
    "\n",
    "emails[\"source\"] = \"email\"\n",
    "\n",
    "# For URLS dataset (usually 'url' for domain, 'Type' or 'label' for class)\n",
    "if \"url\" in urls.columns:\n",
    "    urls.rename(columns={\"url\": \"text\"}, inplace=True)\n",
    "if \"Type\" in urls.columns:\n",
    "    urls.rename(columns={\"Type\": \"label\"}, inplace=True)\n",
    "if \"Label\" in urls.columns:\n",
    "    urls.rename(columns={\"Label\": \"label\"}, inplace=True)\n",
    "\n",
    "urls[\"source\"] = \"url\"\n",
    "\n",
    "# 4. Concatenate both datasets\n",
    "df = pd.concat(\n",
    "    [emails[[\"text\",\"label\",\"source\"]],\n",
    "     urls[[\"text\",\"label\",\"source\"]]],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "#  Normalize labels (so we only have 0=safe, 1=phish)\n",
    "df['label'] = df['label'].replace({\n",
    "    \"Phishing\": 1, \"phishing\": 1, \"malicious\": 1,\n",
    "    \"legitimate\": 0, \"benign\": 0, \"good\": 0\n",
    "})\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "print(\"âœ… Combined dataset ready:\", df.shape)\n",
    "print(df['label'].value_counts())\n",
    "print(df.head())\n",
    "\n",
    "#  Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"text\"], df[\"label\"], test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "#  Vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1,2),\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf  = vectorizer.transform(X_test)\n",
    "\n",
    "#  Logistic Regression\n",
    "model = LogisticRegression(max_iter=2000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "#  Save Model\n",
    "with open(\"phishing_combined_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "#  Save Vocabulary with Python int (convert from numpy.int32)\n",
    "with open(\"vectorizer_vocab.json\", \"w\") as f:\n",
    "    json.dump({k:int(v) for k,v in vectorizer.vocabulary_.items()}, f)\n",
    "\n",
    "print(\"ðŸŽ‰ Model saved as phishing_combined_model.pkl\")\n",
    "print(\"ðŸŽ‰ Vocabulary saved as vectorizer_vocab.json (lightweight)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb857df-1dab-4e58-8c57-ca4f410e53c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
